{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "from shapely.ops import unary_union\n",
    "from shapely.geometry import Point, MultiPolygon, Polygon, LineString\n",
    "import topojson as tp\n",
    "import h3\n",
    "import os\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_time_columns(time_df):\n",
    "    time_df['row_col'] = time_df.apply(lambda row: [row['row'], row['col']], axis=1)\n",
    "    time_df['x_y'] = time_df.apply(lambda row: [row['x'], row['y']], axis=1)\n",
    "    time_df['x_ras_y_ras'] = time_df.apply(lambda row: [row['x_ras'], row['y_ras']], axis=1)\n",
    "    time_df.drop(['row', 'col', 'x', 'y', 'x_ras', 'y_ras'], axis=1, inplace=True)\n",
    "    return time_df\n",
    "\n",
    "def join_points_poly(points_df, poly_df, join_rules):\n",
    "    merged = poly_df\n",
    "    joined_df = gpd.sjoin(points_df, poly_df)\n",
    "    grouped = joined_df.groupby('index_right').agg(join_rules)\n",
    "    merged = merged.merge(grouped, left_index=True, right_index=True)\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in destination files as geodataframes\n",
    "path = './unsynced-data/rwanda/travel-time'\n",
    "\n",
    "# list the files in the path directory and if they end in .parquet, add them to a list called files\n",
    "files = [file for file in os.listdir(path) if file.endswith('.parquet')]\n",
    "\n",
    "datasets = []\n",
    "for file in files:\n",
    "    df = pd.read_parquet(path + '/' + file)\n",
    "    # Convert the DataFrame to a GeoDataFrame\n",
    "    gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df['x'], df['y']))\n",
    "    gdf.crs = 'EPSG:4326'\n",
    "    datasets.append(gdf)\n",
    "\n",
    "# Now datasets list contains GeoDataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting travel_time_to_secondary_schools_fixed.parquet\n",
      "finished with travel_time_to_secondary_schools_fixed.parquet\n",
      "starting travel_time_to_semi_dense_urban_optimal.parquet\n",
      "finished with travel_time_to_semi_dense_urban_optimal.parquet\n",
      "starting travel_time_to_major_hospitals_optimal.parquet\n",
      "finished with travel_time_to_major_hospitals_optimal.parquet\n",
      "starting travel_time_to_health_posts_optimal.parquet\n",
      "finished with travel_time_to_health_posts_optimal.parquet\n",
      "starting travel_time_to_health_centers_optimal.parquet\n",
      "finished with travel_time_to_health_centers_optimal.parquet\n",
      "starting travel_time_to_all_education_facilities_fixed.parquet\n",
      "finished with travel_time_to_all_education_facilities_fixed.parquet\n",
      "starting travel_time_to_primary_schools_fixed.parquet\n",
      "finished with travel_time_to_primary_schools_fixed.parquet\n",
      "starting travel_time_to_all_health_facilities_optimal.parquet\n",
      "finished with travel_time_to_all_health_facilities_optimal.parquet\n"
     ]
    }
   ],
   "source": [
    "# doing some basic stuff with each dataframe\n",
    "# for range of number of lists in datasets print the datasets number\n",
    "\n",
    "for d in range(len(datasets)):\n",
    "    print (\"starting \" +files[d]) \n",
    "    datasets[d] = datasets[d].replace([np.inf, -np.inf], None)\n",
    "    datasets[d] = datasets[d].replace([np.nan], None)\n",
    "    datasets[d] = merge_time_columns(datasets[d])\n",
    "    # appending destination type to travel time and time delta columns\n",
    "    datasets[d] = datasets[d].rename(columns={col: col + files[d][14:-8] for col in datasets[d].columns if col.startswith('travel_time')})\n",
    "    datasets[d] = datasets[d].rename(columns={col: col + files[d][14:-8] for col in datasets[d].columns if col.startswith('time_delta')})\n",
    "    \n",
    "    datasets[d]['row_col'] = datasets[d]['row_col'].astype(str)\n",
    "    print (\"finished with \" +files[d]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging all dataframes into one\n",
    "# each dataframe brings it's unique travel times and travel deltas\n",
    "for d in datasets:\n",
    "    # I make the first df be the main one and then merge all of the rest into it.\n",
    "    travel_time_merged = datasets[0]\n",
    "    for d in range(1, len(datasets)):\n",
    "        # make a list of values from columns that we're going to merge into the dataframe\n",
    "        merge_col = []\n",
    "        for c in list(datasets[d].columns):\n",
    "            if c.startswith('travel_time') or c.startswith('time_delta'):\n",
    "                merge_col.append(c)\n",
    "        # merge the travel_time_merged df with the datasets[d] df on the row_col column\n",
    "        travel_time_merged = pd.merge(travel_time_merged, datasets[d][['row_col'] + merge_col], on='row_col', how='left')\n",
    "        # replace NaN values in travel_time_merged with values from datasets[d] wherever they exist\n",
    "        travel_time_merged = datasets[d][['row_col'] + merge_col].set_index('row_col').combine_first(travel_time_merged.set_index('row_col')).reset_index()\n",
    "        # fill NA values with values from datasets[d]\n",
    "        travel_time_merged = travel_time_merged.fillna(datasets[d])\n",
    "\n",
    "\n",
    "# convert travel_time_merged to geodataframe\n",
    "travel_time_merged = gpd.GeoDataFrame(travel_time_merged, geometry='geometry')\n",
    "travel_time_merged.crs = \"EPSG:4326\"\n",
    "\n",
    "# drop x_ras_y_ras column\n",
    "travel_time_merged = travel_time_merged.drop(['x_ras_y_ras'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "travel_time_merged_proof = travel_time_merged.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change travel_time_merged['x_y'] to a string separated by a _\n",
    "travel_time_merged['x_y'] = travel_time_merged['x_y'].astype(str).str.replace('[', '').str.replace(']', '').str.replace(' ', '').str.replace(',', '_')\n",
    "# change row_col to a string separated by a _\n",
    "travel_time_merged['row_col'] = travel_time_merged['row_col'].astype(str).str.replace('[', '').str.replace(']', '').str.replace(' ', '').str.replace(',', '_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.4384884_-1.0540862\n",
      "10_1897\n"
     ]
    }
   ],
   "source": [
    "print(travel_time_merged['x_y'][0])\n",
    "print(travel_time_merged['row_col'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make rwi equal to rwi times the population value in it's row\n",
    "travel_time_merged['rwi'] = travel_time_merged['rwi'] * travel_time_merged['population']\n",
    "travel_time_merged['underweight'] = travel_time_merged['underweight'] * travel_time_merged['population']\n",
    "# travel_time_merged['male_educational_attainment_mean'] equals the sum the value of male_educational_attainment_mean times the sum of 'males_15_49'+'males_50_64'+'males_65_plus'\n",
    "travel_time_merged['male_educational_attainment_mean'] = travel_time_merged['male_educational_attainment_mean'] * (travel_time_merged['males_15_49']+travel_time_merged['males_50_64']+travel_time_merged['males_65_plus'])\n",
    "travel_time_merged['female_educational_attainment_mean'] = travel_time_merged['female_educational_attainment_mean'] * (travel_time_merged['females_15_49']+travel_time_merged['females_50_64']+travel_time_merged['females_65_plus'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_lists(x):\n",
    "    return \"&\".join(x)\n",
    "    \n",
    "def male_educational_attainment_weighted_average(x):\n",
    "    # Calculate the weighted average using np.average and weights based on sum of males\n",
    "    total_males = travel_time_merged.loc[x.index, ['males_15_49', 'males_50_64', 'males_65_plus']].sum(axis=1)\n",
    "    total_weight = total_males.sum()\n",
    "    if total_weight == 0:\n",
    "        # If the total weight is zero, return a default value (e.g., 0) or handle it as per your requirement.\n",
    "        return 0\n",
    "    non_zero_mask = (total_males > 0) & (~x.isnull()) # Mask to avoid dividing by zero\n",
    "    if non_zero_mask.any():\n",
    "        weighted_avg = np.average(x[non_zero_mask], weights=total_males[non_zero_mask])\n",
    "        return weighted_avg\n",
    "    else:\n",
    "        weighted_avg = np.nan\n",
    "        return weighted_avg\n",
    "def female_educational_attainment_weighted_average(x):\n",
    "    # Calculate the weighted average using np.average and weights based on sum of females\n",
    "    total_females = travel_time_merged.loc[x.index, ['females_15_49', 'females_50_64', 'females_65_plus']].sum(axis=1)\n",
    "    total_weight = total_females.sum()\n",
    "    \n",
    "    if total_weight == 0:\n",
    "        # If the total weight is zero, return a default value (e.g., 0) or handle it as per your requirement.\n",
    "        return 0\n",
    "    \n",
    "    non_zero_mask = (total_females > 0) & (~x.isnull())  # Mask to avoid dividing by zero\n",
    "    if non_zero_mask.any():\n",
    "        weighted_avg = np.average(x[non_zero_mask], weights=total_females[non_zero_mask])\n",
    "        return weighted_avg\n",
    "    else:\n",
    "        weighted_avg = np.nan\n",
    "        return weighted_avg\n",
    "\n",
    "def weighted_average_function(values, weights):\n",
    "    if weights.sum() == 0:\n",
    "        return np.nan\n",
    "    if weights.sum() == np.nan:\n",
    "        return np.nan\n",
    "    if weights.sum() == np.inf:\n",
    "        return np.nan\n",
    "    else:\n",
    "        return np.average(values, weights=weights)\n",
    "\n",
    "\n",
    "joining_rules = {\n",
    "'row_col': aggregate_lists,\n",
    "'x_y': aggregate_lists,\n",
    "'female_educational_attainment_mean': 'sum', \n",
    "'females_0_4': 'sum', \n",
    "'females_0_9': 'sum', \n",
    "'females_10_14': 'sum', \n",
    "'females_15_49': 'sum', \n",
    "'females_50_64': 'sum', \n",
    "'females_5_9': 'sum', \n",
    "'females_65_plus': 'sum', \n",
    "'male_educational_attainment_mean': 'sum',\n",
    "'males_0_4': 'sum', \n",
    "'males_0_9': 'sum', \n",
    "'males_10_14': 'sum', \n",
    "'males_15_49': 'sum', \n",
    "'males_50_64': 'sum', \n",
    "'males_5_9': 'sum', \n",
    "'males_65_plus': 'sum', \n",
    "'pop_0_4': 'sum', \n",
    "'pop_0_9': 'sum', \n",
    "'pop_10_14': 'sum', \n",
    "'pop_15_49': 'sum', \n",
    "'pop_50_64': 'sum', \n",
    "'pop_5_9': 'sum', \n",
    "'pop_65_plus': 'sum', \n",
    "'population': 'sum', \n",
    "'pregnancies': 'sum',\n",
    "'births': 'sum', \n",
    "'rwi': 'sum',\n",
    "'underweight': 'sum', \n",
    "'time_delta_constructed_sites_all_education_facilities_fixed': 'mean', \n",
    "'time_delta_constructed_sites_all_health_facilities_optimal': 'mean', \n",
    "'time_delta_constructed_sites_health_centers_optimal': 'mean', \n",
    "'time_delta_constructed_sites_health_posts_optimal': 'mean', \n",
    "'time_delta_constructed_sites_major_hospitals_optimal': 'mean', \n",
    "'time_delta_constructed_sites_primary_schools_fixed': 'mean', \n",
    "'time_delta_constructed_sites_secondary_schools_fixed': 'mean', \n",
    "'time_delta_constructed_sites_semi_dense_urban_optimal': 'mean', \n",
    "'time_delta_no_sites_all_education_facilities_fixed': 'mean', \n",
    "'time_delta_no_sites_all_health_facilities_optimal': 'mean', \n",
    "'time_delta_no_sites_health_centers_optimal': 'mean', \n",
    "'time_delta_no_sites_health_posts_optimal': 'mean', \n",
    "'time_delta_no_sites_major_hospitals_optimal': 'mean', \n",
    "'time_delta_no_sites_primary_schools_fixed': 'mean', \n",
    "'time_delta_no_sites_secondary_schools_fixed': 'mean', \n",
    "'time_delta_no_sites_semi_dense_urban_optimal': 'mean', \n",
    "'travel_time_all_education_facilities_fixed': 'mean', \n",
    "'travel_time_all_health_facilities_optimal': 'mean', \n",
    "'travel_time_constructed_sites_all_education_facilities_fixed': 'mean', \n",
    "'travel_time_constructed_sites_all_health_facilities_optimal': 'mean', \n",
    "'travel_time_constructed_sites_health_centers_optimal': 'mean', \n",
    "'travel_time_constructed_sites_health_posts_optimal': 'mean', \n",
    "'travel_time_constructed_sites_major_hospitals_optimal': 'mean', \n",
    "'travel_time_constructed_sites_primary_schools_fixed': 'mean', \n",
    "'travel_time_constructed_sites_secondary_schools_fixed': 'mean', \n",
    "'travel_time_constructed_sites_semi_dense_urban_optimal': 'mean', \n",
    "'travel_time_health_centers_optimal': 'mean', \n",
    "'travel_time_health_posts_optimal': 'mean', \n",
    "'travel_time_major_hospitals_optimal': 'mean', \n",
    "'travel_time_no_sites_all_education_facilities_fixed': 'mean', \n",
    "'travel_time_no_sites_all_health_facilities_optimal': 'mean', \n",
    "'travel_time_no_sites_health_centers_optimal': 'mean', \n",
    "'travel_time_no_sites_health_posts_optimal': 'mean', \n",
    "'travel_time_no_sites_major_hospitals_optimal': 'mean', \n",
    "'travel_time_no_sites_primary_schools_fixed': 'mean', \n",
    "'travel_time_no_sites_secondary_schools_fixed': 'mean', \n",
    "'travel_time_no_sites_semi_dense_urban_optimal': 'mean', \n",
    "'travel_time_primary_schools_fixed': 'mean', \n",
    "'travel_time_secondary_schools_fixed': 'mean', \n",
    "'travel_time_semi_dense_urban_optimal': 'mean',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_hexagons(resolution, write_to_file):\n",
    "    # make travel_time_merged_hex a geopandas dataframe that matches travel_time_merged\n",
    "    travel_time_merged_hex = travel_time_merged.copy()\n",
    "    travel_time_merged_hex = gpd.GeoDataFrame(travel_time_merged_hex, geometry='geometry')\n",
    "    travel_time_merged_hex.crs = \"EPSG:4326\"\n",
    "\n",
    "    travel_time_merged_hex['h3-index'] = None\n",
    "\n",
    "    for idx, row in travel_time_merged_hex.iterrows():\n",
    "        try:\n",
    "            lat = row['geometry'].y\n",
    "            lon = row['geometry'].x\n",
    "            travel_time_merged_hex.at[idx, 'h3-index'] = h3.geo_to_h3(lat, lon, resolution)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    # merge all rows that match on h3-index following joining rules\n",
    "    travel_time_merged_hex = travel_time_merged_hex.groupby('h3-index').agg(joining_rules)\n",
    "    travel_time_merged_hex = travel_time_merged_hex.reset_index()\n",
    "    # drop any rows without a h3-index\n",
    "    travel_time_merged_hex = travel_time_merged_hex.dropna(subset=['h3-index'])\n",
    "    # convert h3-index to polygon\n",
    "    travel_time_merged_hex['geometry'] = travel_time_merged_hex['h3-index'].apply(lambda x: h3.h3_to_geo_boundary(x, True))\n",
    "    # make geometry column a polygon\n",
    "    travel_time_merged_hex['geometry'] = travel_time_merged_hex['geometry'].apply(lambda x: Polygon(x))\n",
    "    # convert travel_time_merged_hex to geodataframe\n",
    "    travel_time_merged_hex = gpd.GeoDataFrame(travel_time_merged_hex, geometry='geometry')\n",
    "    travel_time_merged_hex.crs = \"EPSG:4326\"\n",
    "    if write_to_file == True:\n",
    "        travel_time_merged_hex.to_file(\"./synced-data/rwa_travel_time_hex-\"+str(resolution)+\".geojson\", driver='GeoJSON', na='null')\n",
    "    return travel_time_merged_hex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "hex8 = make_hexagons(8, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ToDelete\n",
    "def normalize_by_population(df):\n",
    "    try:\n",
    "        df['rwi'] = df['rwi'] / hex8['population']\n",
    "    except:\n",
    "        df['rwi'] = np.nan\n",
    "    try:\n",
    "        df['underweight'] = df['underweight'] / hex8['population']\n",
    "    except:\n",
    "        df['underweight'] = np.nan\n",
    "    try:\n",
    "        df['male_educational_attainment_mean'] = df['male_educational_attainment_mean'] / (df['males_15_49']+df['males_50_64']+df['males_65_plus'])\n",
    "    except:\n",
    "        df['male_educational_attainment_mean'] = np.nan\n",
    "    try:\n",
    "        df['female_educational_attainment_mean'] = df['female_educational_attainment_mean'] / (df['females_15_49']+df['females_50_64']+df['females_65_plus'])\n",
    "    except:\n",
    "        df['female_educational_attainment_mean'] = np.nan\n",
    "    return df\n",
    "hex8 = normalize_by_population()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n"
     ]
    }
   ],
   "source": [
    "# print type of rwi column of hex8\n",
    "print(type(hex8['population'][0])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing the weighted population columns\n",
    "hex8['rwi'] = hex8.apply(lambda row: row['rwi'] / row['population'] if row['population'] != 0 else np.nan, axis=1)\n",
    "hex8['underweight'] = hex8.apply(lambda row: row['underweight'] / row['population'] if row['population'] != 0 else np.nan, axis=1)\n",
    "hex8['male_educational_attainment_mean'] = hex8.apply(lambda row: row['male_educational_attainment_mean'] / (row['males_15_49'] + row['males_50_64'] + row['males_65_plus']) if (row['males_15_49'] + row['males_50_64'] + row['males_65_plus']) != 0 else np.nan, axis=1)\n",
    "hex8['female_educational_attainment_mean'] = hex8.apply(lambda row: row['female_educational_attainment_mean'] / (row['females_15_49'] + row['females_50_64'] + row['females_65_plus']) if (row['females_15_49'] + row['females_50_64'] + row['females_65_plus']) != 0 else np.nan, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.658925975996027\n",
      "8.966146346597323\n"
     ]
    }
   ],
   "source": [
    "# Checking to see normalizaton worked\n",
    "print(hex8['female_educational_attainment_mean'].max())\n",
    "print(hex8['male_educational_attainment_mean'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h3-index                                           object\n",
      "row_col                                            object\n",
      "x_y                                                object\n",
      "female_educational_attainment_mean                float64\n",
      "females_0_4                                        object\n",
      "                                                   ...   \n",
      "travel_time_no_sites_semi_dense_urban_optimal      object\n",
      "travel_time_primary_schools_fixed                  object\n",
      "travel_time_secondary_schools_fixed                object\n",
      "travel_time_semi_dense_urban_optimal               object\n",
      "geometry                                         geometry\n",
      "Length: 72, dtype: object\n",
      "h3-index                                           object\n",
      "row_col                                            object\n",
      "x_y                                                object\n",
      "female_educational_attainment_mean                float32\n",
      "females_0_4                                         int64\n",
      "                                                   ...   \n",
      "travel_time_no_sites_semi_dense_urban_optimal       int64\n",
      "travel_time_primary_schools_fixed                   int64\n",
      "travel_time_secondary_schools_fixed                 int64\n",
      "travel_time_semi_dense_urban_optimal                int64\n",
      "geometry                                         geometry\n",
      "Length: 72, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# should do this after merging with hexagons so  that we don't lose precision\n",
    "# change datatypes of columns\n",
    "print(hex8.dtypes)\n",
    "int_list = ['time_delta_constructed_sites_all_education_facilities_fixed', 'time_delta_constructed_sites_all_health_facilities_optimal', 'time_delta_constructed_sites_health_centers_optimal', 'time_delta_constructed_sites_health_posts_optimal', 'time_delta_constructed_sites_major_hospitals_optimal', 'time_delta_constructed_sites_primary_schools_fixed', 'time_delta_constructed_sites_secondary_schools_fixed', 'time_delta_constructed_sites_semi_dense_urban_optimal', 'time_delta_no_sites_all_education_facilities_fixed', 'time_delta_no_sites_all_health_facilities_optimal', 'time_delta_no_sites_health_centers_optimal', 'time_delta_no_sites_health_posts_optimal', 'time_delta_no_sites_major_hospitals_optimal', 'time_delta_no_sites_primary_schools_fixed', 'time_delta_no_sites_secondary_schools_fixed', 'time_delta_no_sites_semi_dense_urban_optimal', 'travel_time_all_education_facilities_fixed', 'travel_time_all_health_facilities_optimal', 'travel_time_constructed_sites_all_education_facilities_fixed', 'travel_time_constructed_sites_all_health_facilities_optimal', 'travel_time_constructed_sites_health_centers_optimal', 'travel_time_constructed_sites_health_posts_optimal', 'travel_time_constructed_sites_major_hospitals_optimal', 'travel_time_constructed_sites_primary_schools_fixed', 'travel_time_constructed_sites_secondary_schools_fixed', 'travel_time_constructed_sites_semi_dense_urban_optimal', 'travel_time_health_centers_optimal', 'travel_time_health_posts_optimal', 'travel_time_major_hospitals_optimal', 'travel_time_no_sites_all_education_facilities_fixed', 'travel_time_no_sites_all_health_facilities_optimal', 'travel_time_no_sites_health_centers_optimal', 'travel_time_no_sites_health_posts_optimal', 'travel_time_no_sites_major_hospitals_optimal', 'travel_time_no_sites_primary_schools_fixed', 'travel_time_no_sites_secondary_schools_fixed', 'travel_time_no_sites_semi_dense_urban_optimal', 'travel_time_primary_schools_fixed', 'travel_time_secondary_schools_fixed', 'travel_time_semi_dense_urban_optimal', 'males_0_4', 'males_0_9', 'males_10_14', 'males_15_49', 'males_50_64', 'males_5_9', 'males_65_plus', 'pop_0_4', 'pop_0_9', 'pop_10_14', 'pop_15_49', 'pop_50_64', 'pop_5_9', 'pop_65_plus', 'population', 'females_0_4', 'females_0_9', 'females_10_14', 'females_15_49', 'females_50_64', 'females_5_9', 'females_65_plus', 'births', 'pregnancies', ]\n",
    "float_list = ['rwi', 'underweight', 'female_educational_attainment_mean', 'male_educational_attainment_mean']\n",
    "string_list = ['x_y', 'row_col']\n",
    "for col in int_list:\n",
    "        hex8[col] = hex8['travel_time_semi_dense_urban_optimal'].astype(int)\n",
    "for col in float_list:\n",
    "    hex8[col] = pd.to_numeric(hex8[col], errors='coerce', downcast='float')\n",
    "    hex8[col] = hex8[col].round(4)\n",
    "\n",
    "print(hex8.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop x_y from hex8. We won't use this in the app\n",
    "hex8 = hex8.drop(['x_y'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "hex8.to_file(\"./synced-data/rwa_travel_time_hex-8.geojson\", driver='GeoJSON', na='null')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a hex to subregion lookup table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe with just h3-index and row_col\n",
    "lookup = hex8[['h3-index', 'row_col']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'append'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[114], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m row_col_list \u001b[39m=\u001b[39m row[\u001b[39m'\u001b[39m\u001b[39mrow_col\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m&\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[39mfor\u001b[39;00m row_col \u001b[39min\u001b[39;00m row_col_list:\n\u001b[0;32m----> 6\u001b[0m     lookup_full \u001b[39m=\u001b[39m lookup_full\u001b[39m.\u001b[39;49mappend({\u001b[39m'\u001b[39m\u001b[39mh3-index\u001b[39m\u001b[39m'\u001b[39m: row[\u001b[39m'\u001b[39m\u001b[39mh3-index\u001b[39m\u001b[39m'\u001b[39m], \u001b[39m'\u001b[39m\u001b[39mrow_col\u001b[39m\u001b[39m'\u001b[39m: row_col}, ignore_index\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/geo/lib/python3.10/site-packages/pandas/core/generic.py:5989\u001b[0m, in \u001b[0;36mNDFrame.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5982\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   5983\u001b[0m     name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_internal_names_set\n\u001b[1;32m   5984\u001b[0m     \u001b[39mand\u001b[39;00m name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_metadata\n\u001b[1;32m   5985\u001b[0m     \u001b[39mand\u001b[39;00m name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_accessors\n\u001b[1;32m   5986\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_info_axis\u001b[39m.\u001b[39m_can_hold_identifiers_and_holds_name(name)\n\u001b[1;32m   5987\u001b[0m ):\n\u001b[1;32m   5988\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m[name]\n\u001b[0;32m-> 5989\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mobject\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__getattribute__\u001b[39;49m(\u001b[39mself\u001b[39;49m, name)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'append'"
     ]
    }
   ],
   "source": [
    "# create a new dataframe called lookup_full that has two columns: h3-index and row_col. row_col is the values from each row split on & with the h3-index repeated for each row_col value\n",
    "lookup_full = pd.DataFrame(columns=['h3-index', 'row_col'])\n",
    "for idx, row in lookup.iterrows():\n",
    "    row_col_list = row['row_col'].split('&')\n",
    "    for row_col in row_col_list:\n",
    "        lookup_full = lookup_full.append({'h3-index': row['h3-index'], 'row_col': row_col}, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assuming you have a DataFrame called 'lookup' with columns 'h3-index' and 'row_col'\n",
    "\n",
    "# create a list to store dictionaries for each row in the new DataFrame\n",
    "data = []\n",
    "\n",
    "for idx, row in lookup.iterrows():\n",
    "    row_col_list = row['row_col'].split('&')\n",
    "    for row_col in row_col_list:\n",
    "        data.append({'h3-index': row['h3-index'], 'row_col': row_col})\n",
    "\n",
    "# create the new DataFrame 'lookup_full' using the list of dictionaries\n",
    "lookup_full = pd.DataFrame(data)\n",
    "\n",
    "# If needed, you can set the data types of the columns explicitly, for example:\n",
    "# lookup_full = pd.DataFrame(data, dtype={'h3-index': int, 'row_col': str})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>h3-index</th>\n",
       "      <th>row_col</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>886ad80001fffff</td>\n",
       "      <td>1334_1576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>886ad80001fffff</td>\n",
       "      <td>1335_1573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>886ad80001fffff</td>\n",
       "      <td>1335_1574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>886ad80001fffff</td>\n",
       "      <td>1335_1575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>886ad80001fffff</td>\n",
       "      <td>1335_1576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1150599</th>\n",
       "      <td>886adeb76dfffff</td>\n",
       "      <td>760_600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1150600</th>\n",
       "      <td>886adeb76dfffff</td>\n",
       "      <td>760_601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1150601</th>\n",
       "      <td>886adeb76dfffff</td>\n",
       "      <td>760_602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1150602</th>\n",
       "      <td>886adeb893fffff</td>\n",
       "      <td>974_516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1150603</th>\n",
       "      <td>886adeb893fffff</td>\n",
       "      <td>975_515</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1150604 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                h3-index    row_col\n",
       "0        886ad80001fffff  1334_1576\n",
       "1        886ad80001fffff  1335_1573\n",
       "2        886ad80001fffff  1335_1574\n",
       "3        886ad80001fffff  1335_1575\n",
       "4        886ad80001fffff  1335_1576\n",
       "...                  ...        ...\n",
       "1150599  886adeb76dfffff    760_600\n",
       "1150600  886adeb76dfffff    760_601\n",
       "1150601  886adeb76dfffff    760_602\n",
       "1150602  886adeb893fffff    974_516\n",
       "1150603  886adeb893fffff    975_515\n",
       "\n",
       "[1150604 rows x 2 columns]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lookup_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26138"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many unique values of h3-index are there\n",
    "len(lookup_full['h3-index'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write lookup_full to json file\n",
    "lookup_full.to_json('./synced-data/subregion-to-hex-index.json', orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write lookup_full to csv file\n",
    "lookup_full.to_csv('./synced-data/subregion-to-hex-index.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
